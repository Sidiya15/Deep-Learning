# Deep-Learning

It's well known that labeling all data can be expensive and on top of that, time consuming. Therefore, semi-supervised models are the best tool known nowadays to work around this problem. Generative adversial networks are generative learning methods. Their goal is to train a generator network G(z; θ<sup>(G)</sup>) which produces images of handwritten digits from the data p<sub>data</sub>(x) by transforming vectors of noise z into vectors x = G(z; θ<sup>(G)</sup>). The generic model is trained by a discriminator network D(x), in our case CNN & DNN, which is trained to distinguish between real data and data generated by the generic model p<sub>model</sub>(x). The idea is that the discriminator will serve as a classifier. More specifically, we will have labeled, unlabeled and noise data available. Our GAN will predict 11 classes (10 classes for the ten digits for real data and 1 classe for fake data). Then the generator is in turn trained to fool the discriminator and that exacly where the name <b>Generative Adversial Networks</b> came from.
  
All along this project, we used several technics like <b>Feature matching</b>, <b>Minibatch discrimination</b> and <b>One-sided label smoothing</b> in order to obtain the balance between the generator and the discriminator during the training since each model intends to optimise its own loss function. 
 
The jupyter notebook contains some of the matheamtics that lay behind the conception of those models as well as the associated loss functions. 


